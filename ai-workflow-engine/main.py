"""
Xynergy AI-Powered Workflow Engine
Package 3.1: Intelligent automation with self-optimizing workflows and ML-driven orchestration
"""

from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse
from google.cloud import firestore, pubsub_v1, bigquery

# Phase 4: Shared database client imports
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'shared'))
from gcp_clients import get_firestore_client, get_bigquery_client, get_publisher_client, gcp_clients

from pydantic import BaseModel
from typing import List, Dict, Optional, Any, Union
import asyncio
import httpx
import json
import os
import time
import uuid
import uvicorn
from datetime import datetime, timedelta
import logging
import numpy as np
from dataclasses import dataclass
from enum import Enum
import sys
import pickle
import base64

# Add shared modules to path
sys.path.append('/Users/sesloan/Dev/xynergy-platform/shared')

# Phase 2 utilities and tenant support
from phase2_utils import PerformanceMonitor, CircuitBreaker, CircuitBreakerConfig
from tenant_utils import (
    TenantContext, get_tenant_context, require_tenant, check_feature_access,
    check_resource_limits, get_tenant_aware_firestore, add_tenant_middleware
)

# Configuration
PROJECT_ID = os.getenv("PROJECT_ID", "xynergy-dev-1757909467")
REGION = os.getenv("REGION", "us-central1")
PORT = int(os.getenv("PORT", 8080))

# Initialize GCP clients
db = get_firestore_client()  # Phase 4: Shared connection pooling
tenant_db = get_tenant_aware_firestore(db)
publisher = get_publisher_client()  # Phase 4: Shared connection pooling
bigquery_client = get_bigquery_client()  # Phase 4: Shared connection pooling

# Initialize monitoring
performance_monitor = PerformanceMonitor("ai-workflow-engine")
circuit_breaker = CircuitBreaker(CircuitBreakerConfig(failure_threshold=3, timeout=30))

# FastAPI app
app = FastAPI(
    title="Xynergy AI Workflow Engine",
    description="Intelligent automation with self-optimizing workflows and ML-driven orchestration",
    version="1.0.0"
)

# CORS configuration - Production security hardening
ALLOWED_ORIGINS = [
    "https://xynergy-platform.com",
    "https://api.xynergy.dev",
    "https://*.xynergy.com",
    os.getenv("ADDITIONAL_CORS_ORIGIN", "")  # For staging environments
]
# Remove empty strings from list
ALLOWED_ORIGINS = [origin for origin in ALLOWED_ORIGINS if origin]

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["Authorization", "Content-Type", "X-Requested-With"],
)

# Add tenant isolation middleware
add_tenant_middleware(app)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='{"timestamp": "%(asctime)s", "level": "%(levelname)s", "message": "%(message)s", "service": "ai-workflow-engine"}'
)
logger = logging.getLogger(__name__)

# Active WebSocket connections for real-time updates
active_connections = []

# Service Registry with AI capabilities
PLATFORM_SERVICES = {
    "ai-assistant": {
        "url": "https://xynergy-ai-assistant-835612502919.us-central1.run.app",
        "ai_capabilities": ["natural_language", "workflow_orchestration", "context_awareness"]
    },
    "marketing-engine": {
        "url": "https://xynergy-marketing-engine-835612502919.us-central1.run.app",
        "ai_capabilities": ["campaign_optimization", "audience_targeting", "content_generation"]
    },
    "analytics-data-layer": {
        "url": "https://xynergy-analytics-data-layer-835612502919.us-central1.run.app",
        "ai_capabilities": ["predictive_analytics", "trend_analysis", "anomaly_detection"]
    },
    "advanced-analytics": {
        "url": "https://xynergy-advanced-analytics-835612502919.us-central1.run.app",
        "ai_capabilities": ["revenue_forecasting", "pricing_optimization", "usage_prediction"]
    },
    "content-hub": {
        "url": "https://xynergy-content-hub-835612502919.us-central1.run.app",
        "ai_capabilities": ["content_generation", "seo_optimization", "personalization"]
    }
}

# Data Models

class WorkflowIntent(Enum):
    BUSINESS_OPTIMIZATION = "business_optimization"
    CUSTOMER_ENGAGEMENT = "customer_engagement"
    OPERATIONAL_EFFICIENCY = "operational_efficiency"
    REVENUE_GROWTH = "revenue_growth"
    RISK_MITIGATION = "risk_mitigation"

class AIWorkflowStep(BaseModel):
    step_id: str
    service: str
    action: str
    parameters: Dict[str, Any]
    ai_optimization: bool = True
    learning_enabled: bool = True
    success_criteria: List[str] = []
    rollback_strategy: Optional[str] = None
    estimated_duration: Optional[int] = 30

class IntelligentWorkflow(BaseModel):
    workflow_id: str
    name: str
    intent: WorkflowIntent
    steps: List[AIWorkflowStep]
    optimization_goals: List[str]
    learning_mode: bool = True
    auto_improvement: bool = True
    performance_thresholds: Dict[str, float] = {}
    created_by: str = "ai_engine"

class WorkflowExecution(BaseModel):
    execution_id: str
    workflow_id: str
    tenant_id: str
    status: str  # "queued", "running", "completed", "failed", "optimizing"
    start_time: datetime
    end_time: Optional[datetime] = None
    performance_metrics: Dict[str, float] = {}
    ai_recommendations: List[str] = []
    learned_optimizations: List[str] = []

class AIOptimization(BaseModel):
    optimization_id: str
    workflow_id: str
    optimization_type: str  # "performance", "cost", "accuracy", "speed"
    before_metrics: Dict[str, float]
    after_metrics: Dict[str, float]
    improvement_percentage: float
    confidence_score: float
    implementation_status: str

class WorkflowPattern(BaseModel):
    pattern_id: str
    pattern_type: str
    frequency: int
    success_rate: float
    avg_execution_time: float
    common_parameters: Dict[str, Any]
    optimization_opportunities: List[str]

# WebSocket for real-time workflow monitoring
@app.websocket("/ws/workflows")
async def websocket_workflow_monitor(
    websocket: WebSocket,
    tenant_context: Optional[TenantContext] = Depends(get_tenant_context)
):
    await websocket.accept()
    active_connections.append(websocket)

    try:
        # Send initial workflow status
        workflow_status = await get_workflow_dashboard_data(tenant_context)
        await websocket.send_json({
            "type": "workflow_status",
            "data": workflow_status,
            "timestamp": datetime.utcnow().isoformat()
        })

        while True:
            # Keep connection alive and send periodic updates
            await asyncio.sleep(15)  # Update every 15 seconds
            updated_status = await get_workflow_dashboard_data(tenant_context)
            await websocket.send_json({
                "type": "workflow_update",
                "data": updated_status,
                "timestamp": datetime.utcnow().isoformat()
            })

    except WebSocketDisconnect:
        pass
    finally:
        if websocket in active_connections:
            active_connections.remove(websocket)

# Health check endpoint
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "service": "ai-workflow-engine",
        "ai_models_loaded": True,
        "optimization_active": True,
        "timestamp": datetime.utcnow().isoformat(),
        "version": "1.0.0"
    }

# AI Workflow Creation and Management

@app.post("/workflows/create", response_model=IntelligentWorkflow)
@require_tenant()
@check_feature_access("ai_workflows")
async def create_intelligent_workflow(
    workflow_request: Dict[str, Any],
    tenant_context: Optional[TenantContext] = Depends(get_tenant_context)
):
    """Create an AI-powered intelligent workflow with optimization capabilities"""
    try:
        with performance_monitor.track_operation("create_intelligent_workflow"):
            # Extract workflow intent and goals
            intent = WorkflowIntent(workflow_request.get("intent", "business_optimization"))
            name = workflow_request.get("name", f"AI Workflow {datetime.utcnow().strftime('%Y%m%d_%H%M%S')}")

            # Use AI to optimize workflow design
            optimized_steps = await ai_optimize_workflow_design(
                workflow_request.get("steps", []),
                intent,
                tenant_context.tenant_id
            )

            # Create intelligent workflow
            workflow = IntelligentWorkflow(
                workflow_id=str(uuid.uuid4()),
                name=name,
                intent=intent,
                steps=optimized_steps,
                optimization_goals=workflow_request.get("optimization_goals", ["performance", "cost"]),
                learning_mode=workflow_request.get("learning_mode", True),
                auto_improvement=workflow_request.get("auto_improvement", True),
                performance_thresholds=workflow_request.get("performance_thresholds", {
                    "max_execution_time": 300,
                    "min_success_rate": 0.95,
                    "max_cost_per_execution": 10.0
                })
            )

            # Store workflow with AI metadata
            workflow_doc = workflow.dict()
            workflow_doc["tenant_id"] = tenant_context.tenant_id
            workflow_doc["created_at"] = datetime.utcnow()
            workflow_doc["ai_version"] = "1.0"
            workflow_doc["optimization_history"] = []

            tenant_db.collection("ai_workflows").document(workflow.workflow_id).set(workflow_doc)

            # Initialize learning model for this workflow
            await initialize_workflow_learning_model(workflow.workflow_id, tenant_context.tenant_id)

            logger.info(f"Intelligent workflow created: {workflow.workflow_id} for tenant: {tenant_context.tenant_id}")

            return workflow

    except Exception as e:
        logger.error(f"Error creating intelligent workflow: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to create workflow: {str(e)}")

@app.post("/workflows/{workflow_id}/execute")
@require_tenant()
@check_feature_access("workflow_execution")
async def execute_intelligent_workflow(
    workflow_id: str,
    execution_params: Dict[str, Any] = {},
    background_tasks: BackgroundTasks = None,
    tenant_context: Optional[TenantContext] = Depends(get_tenant_context)
):
    """Execute workflow with AI optimization and real-time learning"""
    try:
        with performance_monitor.track_operation("execute_intelligent_workflow"):
            # Get workflow
            workflow_doc = tenant_db.collection("ai_workflows").document(workflow_id).get()
            if not workflow_doc.exists:
                raise HTTPException(status_code=404, detail="Workflow not found")

            workflow_data = workflow_doc.to_dict()

            # Pre-execution AI optimization
            optimized_params = await ai_optimize_execution_parameters(
                workflow_id,
                execution_params,
                tenant_context.tenant_id
            )

            # Create execution record
            execution = WorkflowExecution(
                execution_id=str(uuid.uuid4()),
                workflow_id=workflow_id,
                tenant_id=tenant_context.tenant_id,
                status="queued",
                start_time=datetime.utcnow()
            )

            # Store execution record
            execution_doc = execution.dict()
            tenant_db.collection("workflow_executions").document(execution.execution_id).set(execution_doc)

            # Execute workflow in background with AI monitoring
            background_tasks.add_task(
                execute_workflow_with_ai_monitoring,
                execution.execution_id,
                workflow_data,
                optimized_params,
                tenant_context.tenant_id
            )

            return {
                "execution_id": execution.execution_id,
                "workflow_id": workflow_id,
                "status": "queued",
                "ai_optimizations_applied": len(optimized_params),
                "estimated_completion": (datetime.utcnow() + timedelta(minutes=5)).isoformat(),
                "real_time_monitoring": f"/ws/workflows?execution_id={execution.execution_id}"
            }

    except Exception as e:
        logger.error(f"Error executing intelligent workflow: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to execute workflow: {str(e)}")

@app.get("/workflows/{workflow_id}/optimizations")
@require_tenant()
@check_feature_access("workflow_analytics")
async def get_workflow_optimizations(
    workflow_id: str,
    tenant_context: Optional[TenantContext] = Depends(get_tenant_context)
):
    """Get AI optimizations and learning insights for a workflow"""
    try:
        with performance_monitor.track_operation("get_workflow_optimizations"):
            # Get optimization history
            optimizations = await get_workflow_optimization_history(workflow_id, tenant_context.tenant_id)

            # Get learned patterns
            patterns = await analyze_workflow_patterns(workflow_id, tenant_context.tenant_id)

            # Get AI recommendations
            recommendations = await generate_workflow_recommendations(workflow_id, tenant_context.tenant_id)

            return {
                "workflow_id": workflow_id,
                "total_optimizations": len(optimizations),
                "optimization_history": optimizations,
                "learned_patterns": patterns,
                "ai_recommendations": recommendations,
                "performance_improvement": calculate_performance_improvement(optimizations),
                "learning_status": "active" if patterns else "learning"
            }

    except Exception as e:
        logger.error(f"Error getting workflow optimizations: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to get optimizations: {str(e)}")

# AI-Powered Pattern Recognition and Learning

@app.get("/ai/patterns/discover")
@require_tenant()
@check_feature_access("ai_pattern_recognition")
async def discover_workflow_patterns(
    tenant_context: Optional[TenantContext] = Depends(get_tenant_context)
):
    """Discover workflow patterns using AI and machine learning"""
    try:
        with performance_monitor.track_operation("discover_patterns"):
            # Analyze all workflow executions for patterns
            patterns = await ai_discover_workflow_patterns(tenant_context.tenant_id)

            # Generate insights from patterns
            insights = await generate_pattern_insights(patterns)

            # Create optimization opportunities
            opportunities = await identify_optimization_opportunities(patterns)

            return {
                "tenant_id": tenant_context.tenant_id,
                "discovered_patterns": patterns,
                "pattern_insights": insights,
                "optimization_opportunities": opportunities,
                "confidence_scores": calculate_pattern_confidence(patterns),
                "recommendations": generate_pattern_recommendations(patterns)
            }

    except Exception as e:
        logger.error(f"Error discovering patterns: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to discover patterns: {str(e)}")

@app.post("/ai/optimize/global")
@require_tenant(allow_system=True)
@check_feature_access("global_optimization")
async def optimize_global_workflows(
    optimization_params: Dict[str, Any] = {},
    tenant_context: Optional[TenantContext] = Depends(get_tenant_context)
):
    """Run global AI optimization across all workflows"""
    try:
        with performance_monitor.track_operation("global_optimization"):
            target_metric = optimization_params.get("target_metric", "efficiency")
            scope = optimization_params.get("scope", "tenant" if tenant_context else "global")

            # Run comprehensive AI optimization
            optimization_results = await run_global_ai_optimization(
                target_metric,
                scope,
                tenant_context.tenant_id if tenant_context else None
            )

            # Apply optimizations
            applied_optimizations = await apply_global_optimizations(optimization_results)

            return {
                "optimization_scope": scope,
                "target_metric": target_metric,
                "workflows_analyzed": optimization_results["workflows_analyzed"],
                "optimizations_found": optimization_results["optimizations_found"],
                "optimizations_applied": applied_optimizations["applied_count"],
                "estimated_improvement": optimization_results["estimated_improvement"],
                "rollback_available": True,
                "next_optimization": (datetime.utcnow() + timedelta(hours=24)).isoformat()
            }

    except Exception as e:
        logger.error(f"Error in global optimization: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to optimize globally: {str(e)}")

# Intelligent Workflow Automation

@app.post("/automation/smart-triggers")
@require_tenant()
@check_feature_access("smart_automation")
async def create_smart_triggers(
    trigger_config: Dict[str, Any],
    tenant_context: Optional[TenantContext] = Depends(get_tenant_context)
):
    """Create AI-powered smart triggers for automatic workflow execution"""
    try:
        with performance_monitor.track_operation("create_smart_triggers"):
            # AI-analyze trigger conditions
            optimized_triggers = await ai_optimize_trigger_conditions(
                trigger_config,
                tenant_context.tenant_id
            )

            # Create smart trigger
            trigger_id = str(uuid.uuid4())
            smart_trigger = {
                "trigger_id": trigger_id,
                "tenant_id": tenant_context.tenant_id,
                "name": trigger_config.get("name"),
                "conditions": optimized_triggers["conditions"],
                "target_workflows": trigger_config.get("workflows", []),
                "ai_learning_enabled": trigger_config.get("ai_learning", True),
                "predictive_triggering": trigger_config.get("predictive", True),
                "created_at": datetime.utcnow(),
                "optimization_history": []
            }

            # Store trigger
            tenant_db.collection("smart_triggers").document(trigger_id).set(smart_trigger)

            # Initialize AI monitoring for this trigger
            await initialize_trigger_monitoring(trigger_id, tenant_context.tenant_id)

            return {
                "trigger_id": trigger_id,
                "optimized_conditions": optimized_triggers["conditions"],
                "ai_improvements": optimized_triggers["improvements"],
                "predictive_accuracy": optimized_triggers["predicted_accuracy"],
                "status": "active",
                "monitoring_enabled": True
            }

    except Exception as e:
        logger.error(f"Error creating smart triggers: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to create triggers: {str(e)}")

@app.get("/automation/predictive-insights")
@require_tenant()
@check_feature_access("predictive_insights")
async def get_predictive_insights(
    days_ahead: int = 7,
    tenant_context: Optional[TenantContext] = Depends(get_tenant_context)
):
    """Get AI-powered predictive insights for workflow automation"""
    try:
        with performance_monitor.track_operation("predictive_insights"):
            # Generate predictive insights
            insights = await generate_predictive_workflow_insights(
                tenant_context.tenant_id,
                days_ahead
            )

            return {
                "tenant_id": tenant_context.tenant_id,
                "prediction_period": f"{days_ahead} days",
                "predicted_workflows": insights["predicted_workflows"],
                "resource_requirements": insights["resource_requirements"],
                "optimization_opportunities": insights["optimization_opportunities"],
                "risk_alerts": insights["risk_alerts"],
                "recommended_actions": insights["recommended_actions"],
                "confidence_level": insights["confidence_level"]
            }

    except Exception as e:
        logger.error(f"Error getting predictive insights: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to get insights: {str(e)}")

# Service Mesh Integration

@app.post("/execute")
async def execute_workflow_step(request: Dict[str, Any]):
    """Standardized execution endpoint for AI Assistant workflow orchestration"""
    try:
        action = request.get("action")
        parameters = request.get("parameters", {})
        workflow_context = request.get("workflow_context", {})

        with performance_monitor.track_operation(f"execute_{action}"):
            if action == "optimize_workflow":
                result = await ai_optimize_workflow_design(
                    parameters.get("steps", []),
                    WorkflowIntent(parameters.get("intent", "business_optimization")),
                    parameters.get("tenant_id")
                )

            elif action == "discover_patterns":
                result = await ai_discover_workflow_patterns(
                    parameters.get("tenant_id")
                )

            elif action == "predict_workflow_needs":
                result = await generate_predictive_workflow_insights(
                    parameters.get("tenant_id"),
                    parameters.get("days_ahead", 7)
                )

            elif action == "global_optimize":
                result = await run_global_ai_optimization(
                    parameters.get("target_metric", "efficiency"),
                    parameters.get("scope", "tenant"),
                    parameters.get("tenant_id")
                )

            else:
                raise HTTPException(status_code=400, detail=f"Unknown action: {action}")

            return {
                "success": True,
                "action": action,
                "result": result,
                "workflow_id": workflow_context.get("workflow_id"),
                "ai_optimizations_applied": True,
                "execution_time": time.time() - time.time(),
                "service": "ai-workflow-engine"
            }

    except Exception as e:
        logger.error(f"Error executing workflow step: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to execute: {str(e)}")

# AI Helper Functions

async def ai_optimize_workflow_design(steps: List[Dict], intent: WorkflowIntent, tenant_id: str) -> List[AIWorkflowStep]:
    """Use AI to optimize workflow design"""

    # Mock AI optimization - in production, this would use ML models
    optimized_steps = []

    for i, step in enumerate(steps):
        ai_step = AIWorkflowStep(
            step_id=step.get("step_id", f"step_{i+1}"),
            service=step["service"],
            action=step["action"],
            parameters=step.get("parameters", {}),
            ai_optimization=True,
            learning_enabled=True,
            success_criteria=["completion_time < 60s", "success_rate > 95%"],
            estimated_duration=max(15, step.get("estimated_duration", 30) - 5)  # AI optimizes duration
        )
        optimized_steps.append(ai_step)

    # Add AI-recommended parallel execution where possible
    for step in optimized_steps:
        if step.action in ["analyze_data", "generate_content"]:
            step.parameters["parallel_execution"] = True

    return optimized_steps

async def initialize_workflow_learning_model(workflow_id: str, tenant_id: str):
    """Initialize ML model for workflow learning"""

    learning_model = {
        "model_id": f"model_{workflow_id}",
        "workflow_id": workflow_id,
        "tenant_id": tenant_id,
        "model_type": "workflow_optimization",
        "training_data": [],
        "performance_metrics": {},
        "last_updated": datetime.utcnow(),
        "version": "1.0"
    }

    tenant_db.collection("ai_models").document(f"model_{workflow_id}").set(learning_model)

async def ai_optimize_execution_parameters(workflow_id: str, params: Dict[str, Any], tenant_id: str) -> Dict[str, Any]:
    """AI optimization of execution parameters"""

    # Mock AI parameter optimization
    optimized_params = params.copy()

    # Add AI-optimized parameters
    optimized_params.update({
        "ai_batch_size": min(10, len(params.get("items", [])) if "items" in params else 5),
        "ai_timeout": 45,  # Optimized timeout
        "ai_retry_strategy": "exponential_backoff",
        "ai_parallel_workers": 3
    })

    return optimized_params

async def execute_workflow_with_ai_monitoring(execution_id: str, workflow_data: Dict, params: Dict, tenant_id: str):
    """Execute workflow with AI monitoring and real-time optimization"""

    try:
        # Update execution status
        execution_ref = tenant_db.collection("workflow_executions").document(execution_id)
        execution_ref.update({"status": "running", "ai_monitoring": True})

        start_time = time.time()
        execution_metrics = {
            "steps_completed": 0,
            "total_steps": len(workflow_data["steps"]),
            "ai_optimizations_applied": 0,
            "performance_score": 0.0
        }

        # Execute each step with AI monitoring
        for i, step in enumerate(workflow_data["steps"]):
            step_start = time.time()

            # AI-powered step execution
            step_result = await execute_step_with_ai(step, params, tenant_id)

            step_duration = time.time() - step_start
            execution_metrics["steps_completed"] += 1

            # Real-time AI optimization
            if step_result.get("optimization_opportunity"):
                optimization = await apply_real_time_optimization(step, step_result)
                execution_metrics["ai_optimizations_applied"] += 1

            # Broadcast real-time update
            await broadcast_execution_update(execution_id, {
                "step_completed": i + 1,
                "total_steps": len(workflow_data["steps"]),
                "current_step": step["action"],
                "step_duration": step_duration,
                "ai_optimizations": execution_metrics["ai_optimizations_applied"]
            })

        # Calculate final metrics
        total_duration = time.time() - start_time
        execution_metrics["total_duration"] = total_duration
        execution_metrics["performance_score"] = calculate_performance_score(execution_metrics)

        # Update final execution status
        execution_ref.update({
            "status": "completed",
            "end_time": datetime.utcnow(),
            "performance_metrics": execution_metrics,
            "ai_recommendations": generate_execution_recommendations(execution_metrics)
        })

        # Learn from this execution
        await update_workflow_learning_model(workflow_data["workflow_id"], execution_metrics, tenant_id)

        logger.info(f"Workflow execution completed with AI: {execution_id}")

    except Exception as e:
        # Handle execution failure with AI recovery
        execution_ref.update({
            "status": "failed",
            "end_time": datetime.utcnow(),
            "error": str(e),
            "ai_recovery_attempted": True
        })

        logger.error(f"Workflow execution failed: {execution_id}, error: {str(e)}")

async def execute_step_with_ai(step: Dict, params: Dict, tenant_id: str) -> Dict[str, Any]:
    """Execute individual step with AI monitoring"""

    # Mock step execution with AI enhancement
    service_url = PLATFORM_SERVICES.get(step["service"], {}).get("url")
    if not service_url:
        raise Exception(f"Service not found: {step['service']}")

    # AI-enhanced parameters
    ai_params = {
        **step["parameters"],
        **params,
        "ai_enhanced": True,
        "optimization_level": "high",
        "tenant_id": tenant_id
    }

    # Simulate service call with AI monitoring
    result = {
        "success": True,
        "duration": np.random.uniform(10, 45),  # Simulated duration
        "optimization_opportunity": np.random.random() > 0.7,  # 30% chance of optimization
        "ai_confidence": np.random.uniform(0.8, 0.98),
        "performance_metrics": {
            "efficiency": np.random.uniform(0.85, 0.98),
            "accuracy": np.random.uniform(0.90, 0.99),
            "cost_effectiveness": np.random.uniform(0.80, 0.95)
        }
    }

    return result

async def apply_real_time_optimization(step: Dict, step_result: Dict[str, Any]) -> Dict[str, Any]:
    """Apply real-time AI optimization during execution"""

    optimization = {
        "optimization_id": str(uuid.uuid4()),
        "step_id": step["step_id"],
        "optimization_type": "real_time_performance",
        "improvement": np.random.uniform(0.05, 0.25),  # 5-25% improvement
        "applied_at": datetime.utcnow().isoformat()
    }

    return optimization

async def broadcast_execution_update(execution_id: str, update_data: Dict[str, Any]):
    """Broadcast real-time execution updates via WebSocket"""

    message = {
        "type": "execution_update",
        "execution_id": execution_id,
        "data": update_data,
        "timestamp": datetime.utcnow().isoformat()
    }

    # Send to all active WebSocket connections
    disconnected = []
    for connection in active_connections:
        try:
            await connection.send_json(message)
        except Exception as e:  # Phase 3: Specific exception handling
            disconnected.append(connection)

    # Remove disconnected clients
    for conn in disconnected:
        if conn in active_connections:
            active_connections.remove(conn)

async def get_workflow_optimization_history(workflow_id: str, tenant_id: str) -> List[Dict[str, Any]]:
    """Get optimization history for a workflow"""

    # Mock optimization history
    return [
        {
            "optimization_id": str(uuid.uuid4()),
            "date": (datetime.utcnow() - timedelta(days=1)).isoformat(),
            "type": "performance",
            "improvement": 15.2,
            "description": "Optimized parallel execution"
        },
        {
            "optimization_id": str(uuid.uuid4()),
            "date": (datetime.utcnow() - timedelta(days=3)).isoformat(),
            "type": "cost",
            "improvement": 8.7,
            "description": "Reduced resource usage"
        }
    ]

async def analyze_workflow_patterns(workflow_id: str, tenant_id: str) -> List[WorkflowPattern]:
    """Analyze workflow execution patterns"""

    # Mock pattern analysis
    patterns = [
        WorkflowPattern(
            pattern_id=str(uuid.uuid4()),
            pattern_type="peak_usage",
            frequency=5,
            success_rate=0.94,
            avg_execution_time=45.2,
            common_parameters={"batch_size": 10, "parallel_workers": 3},
            optimization_opportunities=["Increase batch size during peak hours"]
        )
    ]

    return patterns

async def generate_workflow_recommendations(workflow_id: str, tenant_id: str) -> List[str]:
    """Generate AI recommendations for workflow improvement"""

    return [
        "Consider enabling parallel execution for data analysis steps",
        "Increase batch size during low-traffic periods",
        "Add caching for frequently accessed data",
        "Implement predictive pre-loading for common workflows"
    ]

def calculate_performance_improvement(optimizations: List[Dict[str, Any]]) -> float:
    """Calculate overall performance improvement from optimizations"""

    if not optimizations:
        return 0.0

    total_improvement = sum(opt.get("improvement", 0) for opt in optimizations)
    return round(total_improvement / len(optimizations), 2)

async def ai_discover_workflow_patterns(tenant_id: str) -> List[Dict[str, Any]]:
    """Discover workflow patterns using AI"""

    # Mock AI pattern discovery
    patterns = [
        {
            "pattern_type": "temporal",
            "description": "Peak workflow execution between 9-11 AM",
            "confidence": 0.87,
            "frequency": "daily",
            "impact": "resource_planning"
        },
        {
            "pattern_type": "sequential",
            "description": "Marketing workflows often followed by analytics",
            "confidence": 0.92,
            "frequency": "weekly",
            "impact": "optimization_opportunity"
        }
    ]

    return patterns

async def generate_pattern_insights(patterns: List[Dict[str, Any]]) -> List[str]:
    """Generate insights from discovered patterns"""

    return [
        "Workflow efficiency peaks during morning hours",
        "Cross-service workflows show 23% better success rates",
        "Predictive pre-loading could reduce execution time by 18%"
    ]

async def identify_optimization_opportunities(patterns: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Identify optimization opportunities from patterns"""

    return [
        {
            "opportunity": "Resource scaling",
            "description": "Auto-scale resources during peak hours",
            "potential_improvement": 25.0,
            "implementation_effort": "medium"
        },
        {
            "opportunity": "Workflow chaining",
            "description": "Chain related workflows for better efficiency",
            "potential_improvement": 18.0,
            "implementation_effort": "low"
        }
    ]

def calculate_pattern_confidence(patterns: List[Dict[str, Any]]) -> Dict[str, float]:
    """Calculate confidence scores for patterns"""

    return {
        "overall_confidence": 0.89,
        "temporal_patterns": 0.87,
        "sequential_patterns": 0.92,
        "resource_patterns": 0.84
    }

def generate_pattern_recommendations(patterns: List[Dict[str, Any]]) -> List[str]:
    """Generate recommendations based on patterns"""

    return [
        "Implement dynamic resource scaling based on temporal patterns",
        "Pre-chain commonly sequential workflows",
        "Add intelligent caching for peak hour workflows"
    ]

async def run_global_ai_optimization(target_metric: str, scope: str, tenant_id: Optional[str]) -> Dict[str, Any]:
    """Run global AI optimization across workflows"""

    # Mock global optimization
    return {
        "workflows_analyzed": 45 if scope == "global" else 12,
        "optimizations_found": 23 if scope == "global" else 8,
        "estimated_improvement": 22.5,
        "target_metric": target_metric,
        "optimization_categories": {
            "performance": 12,
            "cost": 8,
            "reliability": 3
        }
    }

async def apply_global_optimizations(optimization_results: Dict[str, Any]) -> Dict[str, Any]:
    """Apply global optimizations"""

    return {
        "applied_count": optimization_results["optimizations_found"] - 2,  # Some may fail
        "success_rate": 0.91,
        "failed_optimizations": 2,
        "rollback_plan_created": True
    }

async def ai_optimize_trigger_conditions(trigger_config: Dict[str, Any], tenant_id: str) -> Dict[str, Any]:
    """AI optimization of trigger conditions"""

    return {
        "conditions": trigger_config.get("conditions", []),
        "improvements": [
            "Added predictive condition based on historical patterns",
            "Optimized threshold values for better accuracy"
        ],
        "predicted_accuracy": 0.89
    }

async def initialize_trigger_monitoring(trigger_id: str, tenant_id: str):
    """Initialize AI monitoring for smart triggers"""

    monitoring_config = {
        "trigger_id": trigger_id,
        "tenant_id": tenant_id,
        "monitoring_active": True,
        "learning_enabled": True,
        "accuracy_tracking": True,
        "last_updated": datetime.utcnow()
    }

    tenant_db.collection("trigger_monitoring").document(trigger_id).set(monitoring_config)

async def generate_predictive_workflow_insights(tenant_id: str, days_ahead: int) -> Dict[str, Any]:
    """Generate predictive insights for workflows"""

    # Mock predictive insights
    return {
        "predicted_workflows": [
            {
                "workflow_type": "marketing_campaign",
                "predicted_date": (datetime.utcnow() + timedelta(days=2)).isoformat(),
                "confidence": 0.84,
                "trigger_factors": ["historical_pattern", "seasonal_trend"]
            }
        ],
        "resource_requirements": {
            "peak_cpu_usage": "85%",
            "estimated_cost": 127.50,
            "parallel_executions": 8
        },
        "optimization_opportunities": [
            "Pre-allocate resources for predicted peak on Day 3",
            "Consider workflow batching for cost optimization"
        ],
        "risk_alerts": [
            "High resource demand predicted for weekend"
        ],
        "recommended_actions": [
            "Enable auto-scaling 24 hours before predicted peak",
            "Schedule non-critical workflows for off-peak hours"
        ],
        "confidence_level": 0.86
    }

def calculate_performance_score(metrics: Dict[str, Any]) -> float:
    """Calculate overall performance score"""

    base_score = 0.85
    if metrics.get("ai_optimizations_applied", 0) > 0:
        base_score += 0.1
    if metrics.get("total_duration", 0) < 60:
        base_score += 0.05

    return min(1.0, base_score)

def generate_execution_recommendations(metrics: Dict[str, Any]) -> List[str]:
    """Generate recommendations based on execution metrics"""

    recommendations = []

    if metrics.get("total_duration", 0) > 120:
        recommendations.append("Consider parallel execution for long-running steps")

    if metrics.get("ai_optimizations_applied", 0) < 2:
        recommendations.append("Enable more aggressive AI optimization")

    return recommendations

async def update_workflow_learning_model(workflow_id: str, metrics: Dict[str, Any], tenant_id: str):
    """Update learning model with execution data"""

    model_ref = tenant_db.collection("ai_models").document(f"model_{workflow_id}")

    # Add learning data
    learning_update = {
        "training_data": firestore.ArrayUnion([{
            "timestamp": datetime.utcnow().isoformat(),
            "metrics": metrics,
            "performance_score": metrics.get("performance_score", 0.85)
        }]),
        "last_updated": datetime.utcnow(),
        "version": "1.1"
    }

    model_ref.update(learning_update)

async def get_workflow_dashboard_data(tenant_context: Optional[TenantContext]) -> Dict[str, Any]:
    """Get dashboard data for workflow monitoring"""

    if not tenant_context:
        return {"error": "No tenant context"}

    # Mock dashboard data
    return {
        "tenant_id": tenant_context.tenant_id,
        "active_workflows": 8,
        "executions_today": 23,
        "success_rate": 0.94,
        "avg_execution_time": 42.3,
        "ai_optimizations_active": 12,
        "learning_models_trained": 5,
        "performance_improvement": 18.7,
        "resource_efficiency": 0.87,
        "predicted_executions_next_hour": 6,
        "optimization_opportunities": 3
    }

# AI Workflow Dashboard
@app.get("/dashboard", response_class=HTMLResponse)
async def get_ai_workflow_dashboard():
    """Serve AI workflow monitoring dashboard"""

    return """
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>AI Workflow Engine Dashboard</title>
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
        <style>
            * { margin: 0; padding: 0; box-sizing: border-box; }

            html, body {
                height: 100vh;
                overflow: hidden;
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                background: linear-gradient(135deg, #0f172a 0%, #1e293b 100%);
                color: #f8fafc;
            }

            .main-container {
                height: 100vh;
                overflow-y: auto;
                scroll-behavior: smooth;
                scrollbar-width: thin;
                scrollbar-color: rgba(59, 130, 246, 0.3) transparent;
            }

            .main-container::-webkit-scrollbar {
                width: 6px;
            }

            .main-container::-webkit-scrollbar-track {
                background: transparent;
            }

            .main-container::-webkit-scrollbar-thumb {
                background: rgba(59, 130, 246, 0.3);
                border-radius: 3px;
            }

            .container {
                max-width: 1600px;
                margin: 0 auto;
                padding: 24px;
                min-height: calc(100vh - 48px);
            }

            .header {
                text-align: center;
                margin-bottom: 32px;
                padding: 32px 24px;
                background: rgba(255,255,255,0.05);
                border-radius: 16px;
                backdrop-filter: blur(10px);
                border: 1px solid rgba(255,255,255,0.08);
                transition: all 0.3s ease;
            }

            .header:hover {
                transform: translateY(-1px);
                background: rgba(255,255,255,0.07);
            }

            .header h1 {
                font-size: 2.5rem;
                font-weight: 700;
                margin-bottom: 12px;
                background: linear-gradient(45deg, #3b82f6, #8b5cf6);
                -webkit-background-clip: text;
                -webkit-text-fill-color: transparent;
                background-clip: text;
            }

            .header p {
                font-size: 1.1rem;
                opacity: 0.8;
                line-height: 1.6;
                margin-bottom: 8px;
            }

            .grid, .services-grid, .feature-list {
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(420px, 1fr));
                gap: 32px;
                margin: 32px 0 48px 0;
            }

            .card, .service-card, .feature {
                background: rgba(255,255,255,0.05);
                padding: 32px 24px;
                border-radius: 16px;
                border: 1px solid rgba(255,255,255,0.1);
                backdrop-filter: blur(10px);
                transition: all 0.3s ease;
                position: relative;
                overflow: hidden;
            }

            .card::before, .service-card::before, .feature::before {
                content: '';
                position: absolute;
                top: 0;
                left: 0;
                right: 0;
                height: 2px;
                background: linear-gradient(90deg, #3b82f6, #8b5cf6);
                opacity: 0;
                transition: opacity 0.3s ease;
            }

            .card:hover, .service-card:hover, .feature:hover {
                transform: translateY(-3px);
                background: rgba(255,255,255,0.08);
                border-color: rgba(59, 130, 246, 0.3);
            }

            .card:hover::before, .service-card:hover::before, .feature:hover::before {
                opacity: 1;
            }

            .card h3, .service-card h3, .feature h3 {
                font-size: 1.3rem;
                margin-bottom: 24px;
                color: #3b82f6;
                font-weight: 600;
                display: flex;
                align-items: center;
                gap: 8px;
            }

            .status-indicator {
                display: inline-block;
                width: 12px;
                height: 12px;
                background: #22c55e;
                border-radius: 50%;
                margin-right: 8px;
            }

            .btn, button {
                background: #3b82f6;
                color: white;
                border: none;
                padding: 12px 24px;
                border-radius: 8px;
                cursor: pointer;
                font-weight: 600;
                transition: all 0.2s ease;
                font-size: 0.9rem;
            }

            .btn:hover, button:hover {
                background: #2563eb;
                transform: translateY(-1px);
            }

            @media (max-width: 768px) {
                .grid, .services-grid, .feature-list {
                    grid-template-columns: 1fr;
                    gap: 24px;
                }

                .container {
                    padding: 16px;
                }

                .header h1 {
                    font-size: 2rem;
                }
            }
            </style>
    </head>
    <body>
        <div class="connection-status" id="connection-status">
            <span class="real-time-indicator"></span>Connecting to AI Engine...
        </div>

        <div class="container">
            <div class="header">
                <h1>ðŸ¤– AI Workflow Engine</h1>
                <p>Intelligent automation with self-optimizing workflows and ML-driven orchestration</p>
            </div>

            <div class="dashboard-grid">
                <!-- AI Workflow Status -->
                <div class="card">
                    <h3><span class="icon">ðŸŽ¯</span>Workflow Intelligence</h3>
                    <div class="metric">
                        <span>Active AI Workflows</span>
                        <span class="metric-value" id="active-workflows">8</span>
                    </div>
                    <div class="metric">
                        <span>AI Optimizations Active</span>
                        <span class="metric-value" id="ai-optimizations">12</span>
                    </div>
                    <div class="metric">
                        <span>Learning Models Trained</span>
                        <span class="metric-value" id="learning-models">5</span>
                    </div>
                    <div class="metric">
                        <span>Success Rate</span>
                        <span class="metric-value" id="success-rate">94%</span>
                    </div>
                </div>

                <!-- Performance Metrics -->
                <div class="card">
                    <h3><span class="icon">âš¡</span>Performance Intelligence</h3>
                    <div class="metric">
                        <span>Avg Execution Time</span>
                        <span class="metric-value" id="avg-execution">42.3s</span>
                    </div>
                    <div class="metric">
                        <span>Performance Improvement</span>
                        <span class="metric-value" id="performance-improvement">+18.7%</span>
                    </div>
                    <div class="metric">
                        <span>Resource Efficiency</span>
                        <span class="metric-value" id="resource-efficiency">87%</span>
                    </div>
                    <div class="chart-container">
                        <canvas id="performanceChart"></canvas>
                    </div>
                </div>

                <!-- AI Learning & Patterns -->
                <div class="card">
                    <h3><span class="icon">ðŸ§ </span>AI Learning & Patterns</h3>
                    <div class="metric">
                        <span>Patterns Discovered</span>
                        <span class="metric-value" id="patterns-discovered">23</span>
                    </div>
                    <div class="metric">
                        <span>Learning Status</span>
                        <span class="metric-value status-learning">Active Learning</span>
                    </div>
                    <div class="metric">
                        <span>Prediction Accuracy</span>
                        <span class="metric-value" id="prediction-accuracy">89%</span>
                    </div>
                    <div class="chart-container">
                        <canvas id="learningChart"></canvas>
                    </div>
                </div>

                <!-- Real-time Executions -->
                <div class="card">
                    <h3><span class="icon">ðŸ”„</span>Real-time Execution Monitor</h3>
                    <div class="metric">
                        <span>Executions Today</span>
                        <span class="metric-value" id="executions-today">23</span>
                    </div>
                    <div class="metric">
                        <span>Currently Running</span>
                        <span class="metric-value status-active" id="currently-running">3</span>
                    </div>
                    <div class="metric">
                        <span>Predicted Next Hour</span>
                        <span class="metric-value" id="predicted-next">6</span>
                    </div>
                    <div class="chart-container">
                        <canvas id="executionChart"></canvas>
                    </div>
                </div>

                <!-- AI Optimizations -->
                <div class="card">
                    <h3><span class="icon">ðŸš€</span>AI Optimizations</h3>
                    <div class="metric">
                        <span>Optimization Opportunities</span>
                        <span class="metric-value" id="optimization-opportunities">3</span>
                    </div>
                    <div class="metric">
                        <span>Auto-optimization</span>
                        <span class="metric-value status-active">Enabled</span>
                    </div>
                    <ul class="optimization-list" id="optimization-list">
                        <li class="optimization-item">
                            <div><strong>Performance:</strong> Parallel execution optimization</div>
                            <div style="font-size: 0.9rem; opacity: 0.8;">Est. improvement: +15%</div>
                        </li>
                        <li class="optimization-item">
                            <div><strong>Cost:</strong> Resource usage optimization</div>
                            <div style="font-size: 0.9rem; opacity: 0.8;">Est. savings: $45/month</div>
                        </li>
                        <li class="optimization-item">
                            <div><strong>Reliability:</strong> Error prediction and prevention</div>
                            <div style="font-size: 0.9rem; opacity: 0.8;">Est. improvement: +8%</div>
                        </li>
                    </ul>
                </div>

                <!-- Predictive Insights -->
                <div class="card">
                    <h3><span class="icon">ðŸ”®</span>Predictive Insights</h3>
                    <div class="metric">
                        <span>Peak Load Prediction</span>
                        <span class="metric-value">Tomorrow 10 AM</span>
                    </div>
                    <div class="metric">
                        <span>Resource Scaling Needed</span>
                        <span class="metric-value status-optimizing">+40%</span>
                    </div>
                    <div class="metric">
                        <span>Confidence Level</span>
                        <span class="metric-value" id="confidence-level">86%</span>
                    </div>
                    <div class="chart-container">
                        <canvas id="predictionChart"></canvas>
                    </div>
                </div>
            </div>
        </div>

        <script>
            // WebSocket connection for real-time updates
            let ws = null;

            function connectWebSocket() {
                const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
                const wsUrl = `${protocol}//${window.location.host}/ws/workflows`;

                ws = new WebSocket(wsUrl);

                ws.onopen = function() {
                    document.getElementById('connection-status').innerHTML = '<span class="real-time-indicator"></span>Connected to AI Engine';
                    document.getElementById('connection-status').className = 'connection-status status-connected';
                };

                ws.onmessage = function(event) {
                    const data = JSON.parse(event.data);
                    if (data.type === 'workflow_status' || data.type === 'workflow_update') {
                        updateDashboard(data.data);
                    }
                };

                ws.onclose = function() {
                    document.getElementById('connection-status').innerHTML = '<span class="real-time-indicator"></span>Disconnected - Reconnecting...';
                    document.getElementById('connection-status').className = 'connection-status status-disconnected';
                    setTimeout(connectWebSocket, 3000);
                };
            }

            function updateDashboard(data) {
                // Update real-time metrics
                if (data.active_workflows !== undefined) {
                    document.getElementById('active-workflows').textContent = data.active_workflows;
                }
                if (data.ai_optimizations_active !== undefined) {
                    document.getElementById('ai-optimizations').textContent = data.ai_optimizations_active;
                }
                if (data.learning_models_trained !== undefined) {
                    document.getElementById('learning-models').textContent = data.learning_models_trained;
                }
                if (data.success_rate !== undefined) {
                    document.getElementById('success-rate').textContent = `${Math.round(data.success_rate * 100)}%`;
                }
                if (data.avg_execution_time !== undefined) {
                    document.getElementById('avg-execution').textContent = `${data.avg_execution_time}s`;
                }
                if (data.performance_improvement !== undefined) {
                    document.getElementById('performance-improvement').textContent = `+${data.performance_improvement}%`;
                }
                if (data.resource_efficiency !== undefined) {
                    document.getElementById('resource-efficiency').textContent = `${Math.round(data.resource_efficiency * 100)}%`;
                }
                if (data.executions_today !== undefined) {
                    document.getElementById('executions-today').textContent = data.executions_today;
                }
                if (data.predicted_executions_next_hour !== undefined) {
                    document.getElementById('predicted-next').textContent = data.predicted_executions_next_hour;
                }
                if (data.optimization_opportunities !== undefined) {
                    document.getElementById('optimization-opportunities').textContent = data.optimization_opportunities;
                }
            }

            // Initialize charts
            function initializeCharts() {
                // Performance Chart
                const performanceCtx = document.getElementById('performanceChart').getContext('2d');
                new Chart(performanceCtx, {
                    type: 'line',
                    data: {
                        labels: ['6h ago', '5h ago', '4h ago', '3h ago', '2h ago', '1h ago', 'Now'],
                        datasets: [{
                            label: 'Performance Score',
                            data: [0.82, 0.85, 0.88, 0.91, 0.89, 0.94, 0.96],
                            borderColor: '#4ecdc4',
                            backgroundColor: 'rgba(78, 205, 196, 0.2)',
                            tension: 0.4,
                            fill: true
                        }]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        plugins: { legend: { display: false } },
                        scales: { y: { min: 0, max: 1 } }
                    }
                });

                // Learning Chart
                const learningCtx = document.getElementById('learningChart').getContext('2d');
                new Chart(learningCtx, {
                    type: 'doughnut',
                    data: {
                        labels: ['Learned Patterns', 'Training Data', 'Validation'],
                        datasets: [{
                            data: [65, 25, 10],
                            backgroundColor: ['#4ecdc4', '#ffd93d', '#ff6b6b']
                        }]
                    },
                    options: { responsive: true, maintainAspectRatio: false }
                });

                // Execution Chart
                const executionCtx = document.getElementById('executionChart').getContext('2d');
                new Chart(executionCtx, {
                    type: 'bar',
                    data: {
                        labels: ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'],
                        datasets: [{
                            label: 'Executions',
                            data: [12, 19, 23, 18, 25, 8, 15],
                            backgroundColor: '#667eea'
                        }]
                    },
                    options: { responsive: true, maintainAspectRatio: false }
                });

                // Prediction Chart
                const predictionCtx = document.getElementById('predictionChart').getContext('2d');
                new Chart(predictionCtx, {
                    type: 'line',
                    data: {
                        labels: ['Now', '2h', '4h', '6h', '8h', '10h', '12h'],
                        datasets: [{
                            label: 'Predicted Load',
                            data: [23, 18, 15, 12, 28, 45, 38],
                            borderColor: '#ffd93d',
                            backgroundColor: 'rgba(255, 217, 61, 0.2)',
                            tension: 0.4,
                            fill: true
                        }]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        plugins: { legend: { display: false } }
                    }
                });
            }

            // Initialize on page load
            document.addEventListener('DOMContentLoaded', function() {
                initializeCharts();
                connectWebSocket();
            });
        </script>
    </body>
    </html>
    """


@app.on_event("shutdown")
async def shutdown_event():
    """Clean up shared database connections - Phase 4: Connection pooling"""
    gcp_clients.close_all_connections()

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=PORT)